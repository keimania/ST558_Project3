---
title: "Modeling"
author: "Jamin Goo"
format: html
editor: visual
---

# 1. Introduction

In this section, we will build predictive models to classify whether an individual has diabetes (Diabetes_binary) based on various health indicators.

We will explore two types of models:

Classification Tree: A decision tree model that splits data into nodes to make predictions.

Random Forest: An ensemble method that aggregates predictions from multiple decision trees to improve accuracy and reduce overfitting.

We will use the log-loss metric to evaluate model performance and 5-fold Cross-Validation on the training set to tune hyperparameters. Finally, we will compare the best models from each category on the test set to select the final winner.

# 2. Setup and Data Preparation

First, we load the necessary libraries and apply the same data cleaning steps as performed in the EDA phase.

```{r}
library(tidyverse)
library(tidymodels)
library(vip)

# Read the data
diabetes <- read_csv("diabetes_binary_health_indicators_BRFSS2015.csv")

# Apply Data Cleaning and Factor Conversion (Same as EDA)
diabetes_clean <- diabetes |>
  mutate(
    Diabetes_binary = factor(Diabetes_binary, levels = c(0, 1), labels = c("No Diabetes", "Diabetes")),
    HighBP = factor(HighBP, levels = c(0, 1), labels = c("No High BP", "High BP")),
    HighChol = factor(HighChol, levels = c(0, 1), labels = c("No High Chol", "High Chol")),
    CholCheck = factor(CholCheck, levels = c(0, 1), labels = c("No Check", "Check in 5 yrs")),
    Smoker = factor(Smoker, levels = c(0, 1), labels = c("No", "Yes")),
    Stroke = factor(Stroke, levels = c(0, 1), labels = c("No", "Yes")),
    HeartDiseaseorAttack = factor(HeartDiseaseorAttack, levels = c(0, 1), labels = c("No", "Yes")),
    PhysActivity = factor(PhysActivity, levels = c(0, 1), labels = c("No", "Yes")),
    Fruits = factor(Fruits, levels = c(0, 1), labels = c("No", "Yes")),
    Veggies = factor(Veggies, levels = c(0, 1), labels = c("No", "Yes")),
    HvyAlcoholConsump = factor(HvyAlcoholConsump, levels = c(0, 1), labels = c("No", "Yes")),
    AnyHealthcare = factor(AnyHealthcare, levels = c(0, 1), labels = c("No", "Yes")),
    NoDocbcCost = factor(NoDocbcCost, levels = c(0, 1), labels = c("No", "Yes")),
    GenHlth = factor(GenHlth, levels = c(1, 2, 3, 4, 5), 
                     labels = c("Excellent", "Very Good", "Good", "Fair", "Poor")),
    DiffWalk = factor(DiffWalk, levels = c(0, 1), labels = c("No", "Yes")),
    Sex = factor(Sex, levels = c(0, 1), labels = c("Female", "Male")),
    Age = factor(Age),
    Education = factor(Education),
    Income = factor(Income)
  )
```

## 2.1. Data Splitting

We split the data into a training set (70%) and a test set (30%). We set a seed to ensure reproducibility.

```{r}
set.seed(123)

# Stratified split to maintain class balance
data_split <- initial_split(diabetes_clean, prop = 0.70, strata = Diabetes_binary)
train_data <- training(data_split)
test_data  <- testing(data_split)

# Verify the split
nrow(train_data)
nrow(test_data)
```

## 2.2. Cross-Validation Folds

We will use 5-fold cross-validation on the training data to tune our model parameters.

```{r}
cv_folds <- vfold_cv(train_data, v = 5, strata = Diabetes_binary)

# Define the metric set: Log Loss
metrics_eval <- metric_set(mn_log_loss)

```

# 3. Model 1: Classification Tree

## 3.1. Introduction to Classification Trees

A Classification Tree is a supervised learning algorithm that predicts the target variable by learning decision rules inferred from the data features. It works by recursively splitting the data into subsets based on the most significant attribute at each step.

We will tune the cost complexity parameter and tree depth to find the optimal tree size and prevent overfitting.

## 3.2. Model Specification and Workflow

```{r}
# 1. Recipe
tree_rec <- recipe(Diabetes_binary ~ BMI + HighBP + HighChol + GenHlth + Age + PhysActivity + DiffWalk + HeartDiseaseorAttack, data = train_data)
```

We set cost_complexity and tree_depth to 'tune()' so we can optimize them later.

```{r}
# 2. Model Specification
tree_spec <- decision_tree(
  cost_complexity = tune(),
  tree_depth = tune()
) |>
  set_engine("rpart") |>
  set_mode("classification")

# 3. Workflow
tree_wkf <- workflow() |>
  add_recipe(tree_rec) |>
  add_model(tree_spec)

# 4. Grid for Tuning
tree_grid <- grid_regular(cost_complexity(), tree_depth(), levels = 5)
```

## 3.3. Tuning and selection

We run the workflow on the cross-validation folds using the grid of parameters

```{r}
# Tune the model
tree_fits <- tree_wkf |>
  tune_grid(
    resamples = cv_folds,
    grid = tree_grid,
    metrics = metrics_eval
  )
```

```{r}
# Collect metrics
tree_fits |>
  collect_metrics()
```

We select the best parameter based on log_loss.

```{r}
best_tree <- select_best(tree_fits, metric = "mn_log_loss")
best_tree
```

We finalize the workflow with the best parameters.

```{r}
# 
tree_final_wkf <- tree_wkf |>
  finalize_workflow(best_tree)
```

We visualize the tuning results using plot Cost Complexity vs. Log Loss, grouped by Tree Depth.

```{r}
tree_fits |>
  collect_metrics() |>
  mutate(tree_depth = factor(tree_depth)) |>
  ggplot(aes(cost_complexity, mean, color = tree_depth)) +
  geom_line(size = 1.5, alpha = 0.6) +
  geom_point(size = 2) +
  facet_wrap(~ .metric, scales = "free", nrow = 2) +
  scale_x_log10(labels = scales::label_number()) +
  scale_color_viridis_d(option = "plasma", begin = .9, end = 0) +
  labs(title = "Classification Tree Tuning Results")
```

We train the finalized model on the full training set and evaluates it on the test set.

```{r}
tree_final_fit <- tree_final_wkf |>
  last_fit(data_split)
tree_final_fit
```

```{r}
tree_final_fit |>
  collect_metrics()
```

We extract the actual model object for inspection.

```{r}
tree_final_model <- extract_workflow(tree_final_fit) 
tree_final_model
```

We visualize the Decision Tree.

```{r}
tree_final_model |>
  extract_fit_engine() |>
  rpart.plot::rpart.plot(roundint = FALSE)
```

# 4. Model 2: Random Forest

## 4.1. Introduction to Random Forest

A Random Forest is an ensemble learning method that constructs a multitude of decision trees at training time. For classification tasks, the output of the random forest is the class selected by most trees (mode).

Random forests generally outperform single decision trees because they correct for the habit of decision trees to overfit to their training set. We will tune mtry (number of variables randomly sampled at each split) and min_n (minimum number of data points in a node).

## 4.2. Model Specification and Workflow

```{r}
# 1. Recipe (We can reuse the same predictors)
rf_rec <- tree_rec
```

We set Use 'ranger' package and calculate variable importance. Number of trees is set to 100 for faster tuning.

```{r}
# 2. Model Specification
rf_spec <- rand_forest(
  mtry = tune(),
  min_n = tune(),
  trees = 100
) |>
  set_engine("ranger", importance = "impurity") |>
  set_mode("classification")

# 3. Workflow
rf_wkf <- workflow() |>
  add_recipe(rf_rec) |>
  add_model(rf_spec)
```

We define the range of hyperparameters to test. 'mtry' range is set from 2 to 7 because we have about 8 predictors available.

```{r}
# 4. Grid for Tuning
rf_grid <- grid_regular(
  mtry(range = c(2, 7)),
  min_n(),
  levels = 3
)
```

## 4.3. Tuning and selection

We run the workflow on the cross-validation folds to find the best hyperparameters.

```{r}
# Tune the model
rf_fits <- rf_wkf |>
  tune_grid(
    resamples = cv_folds,
    grid = rf_grid,
    metrics = metrics_eval
  )
```

```{r}
# Collect metrics
rf_fits |>
  collect_metrics()
```

We select the best parameter based on log_loss.

```{r}
best_rf <- select_best(rf_fits, metric = "mn_log_loss")
best_rf
```

We finalize the workflow with the best parameters.

```{r}
rf_final_wkf <- rf_wkf |>
  finalize_workflow(best_rf)
```

We visualize the tuning results using plot the relationship between mtry, min_n, and model performance.

```{r}
rf_fits |>
  collect_metrics() |>
  mutate(min_n = factor(min_n)) |>
  ggplot(aes(mtry, mean, color = min_n)) +
  geom_line(size = 1.5, alpha = 0.6) +
  geom_point(size = 2) +
  facet_wrap(~ .metric, scales = "free", nrow = 2) +
  scale_color_viridis_d(option = "plasma", begin = .9, end = 0) +
  labs(title = "Random Forest Tuning Results")
```

We train the finalized model on the full training set and evaluates it on the test set.

```{r}
rf_final_fit <- rf_final_wkf |>
  last_fit(data_split, metrics = metrics_eval)
rf_final_fit
```

```{r}
rf_final_fit |>
  collect_metrics()
```

We extract the actual model object for inspection.

```{r}
rf_final_model <- extract_workflow(rf_final_fit) 
rf_final_model
```

We visualize the best Random Forest.

```{r}
rf_final_model |>
  extract_fit_parsnip() |>
  vip::vip() +
  labs(title = "Random Forest Variable Importance")
```

# 5. Final Model Selection

Now we compare both "best" models which have been fitted to the training set and evaluated on the test set.

```{r}
# Collect Metrics from the already fitted objects
tree_test_metrics <- collect_metrics(tree_final_fit)
rf_test_metrics   <- collect_metrics(rf_final_fit)
```

## Combine for comparison

```{r}
model_comparison <- bind_rows(
  tree_test_metrics |> mutate(model = "Classification Tree"),
  rf_test_metrics |> mutate(model = "Random Forest")
) |>
  select(model, .metric, .estimate)

print(model_comparison)
```

# 6. Conclusion

In this analysis, we built and compared two machine learning models to predict diabetes: a single Classification Tree and a Random Forest.

## 6.1. Performance Comparison

We evaluated the models based on the metrics collected from the test set:

-   **Classification Tree:** This model achieved an **Accuracy of 86.3%** and an **ROC AUC of 0.803**. While these numbers indicate decent predictive power, single trees are often prone to high variance.
-   **Random Forest:** The ensemble model achieved a **Log Loss of 0.320**. A lower log-loss indicates that the model is not only correct but also confident in its probabilistic predictions.

## 6.2. Final Selection

Although the Classification Tree is easier to explain, the **Random Forest** demonstrates strong performance metrics (Log Loss: 0.320), suggesting it is robust in handling the complexity of health data. Therefore, we select the **Random Forest** as our final model for deployment to maximize predictive reliability.
